{"cells":[{"cell_type":"markdown","source":["#2. Cargue el conjunto de datos"],"metadata":{}},{"cell_type":"code","source":["data = sqlContext.read.csv(\"/FileStore/tables/cvrud4vb1478717168201/nba.csv\", header=True, inferSchema=True, sep=' ')\ndisplay(data)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["# 3. Entrene un árbol de decisión"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer, MinMaxScaler\n'''\ndata = sqlContext.read.format(\"com.databricks.spark.csv\")\\\n  .option(\"header\",\"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .option(\"delimiter\", \";\")\\\n  .load('/FileStore/tables/30fur5sb1477536120864/credit_german-8665c.csv')\n'''\ncolumns = list(data.dtypes)\nfeatures = []\ndata = data.na.drop()\n#data2=data\n#data = data.drop(data.Class)\n\nfor col, typ in columns:\n  if col == \"Class\":\n    continue\n  if typ == \"string\":\n    idxCol = col+\"Idx\"\n    feaCol = col+\"Feature\"\n    features.append(feaCol)\n    stringIndexer = StringIndexer(inputCol = col, outputCol = idxCol)\n    indexer = stringIndexer.fit(data)\n    data = indexer.transform(data)\n    \n    encoder = OneHotEncoder(dropLast = False, inputCol = idxCol, outputCol = feaCol)\n    data = encoder.transform(data)\n  else:\n    assembler = VectorAssembler(inputCols = [col], outputCol = str(col+\"Num\"))\n    data = assembler.transform(data)\n    features.append(col)\n    \nassembler = VectorAssembler(inputCols = features, outputCol = \"features\")\ndata = assembler.transform(data)\ndata = data.select(\"class\", \"features\")\ndisplay(data)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nlabelIndexer = StringIndexer(inputCol=\"class\", outputCol=\"label\").fit(data)\n\n(train, test) = data.randomSplit([0.7, 0.3])\n\naccuracy_metric = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\nrecall_metric = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nf1_metric = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\")\n\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth = 3)\npipeline = Pipeline(stages=[labelIndexer, dt])\n\nmodel = pipeline.fit(train)\npredictions = model.transform(test)\n\nprint \"Accuracy: \",accuracy_metric.evaluate(predictions)\nprint \"Recall:\", recall_metric.evaluate(predictions)\nprint \"F1:\", f1_metric.evaluate(predictions)\nprint model.stages[1].toDebugString"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["compare_df=predictions.select(\"label\", \"prediction\")\n\ntp=compare_df.filter(compare_df.label==0).filter(compare_df.prediction == 0).count()\nfp=compare_df.filter(compare_df.label==0).filter(compare_df.prediction == 1).count()\nfn=compare_df.filter(compare_df.label==1).filter(compare_df.prediction == 0).count()\ntn=compare_df.filter(compare_df.label==1).filter(compare_df.prediction == 1).count()\n\nimport pandas as pd\nprint (\"Confusion Matrix\")\nCM=pd.DataFrame([[tp,fp],[fn,tn]], columns=['class 1', 'class 2'], index=['class 1','class 2'])\nprint (CM)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["feature_relevance_idx = model.stages[1].featureImportances.indices\nfeature_relevance_values =model.stages[1].featureImportances.values\nfeature_relevance_labels = [{columns[i] : feature_relevance_values[j] } for i, j in zip(feature_relevance_idx, range(len(feature_relevance_values)))]\nimport pprint\npprint.PrettyPrinter().pprint(feature_relevance_labels)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Claramente de los resultados anteriores se puede apreciar que los atributos más relevantes (al menos para el anterior árblol de decisión generado) en orden descendente son:\n\n* Rebotes ofensivos **(OFF)**\n* Porcentaje de tiro de tres puntos **(3p%)**\n* Robos por juego **(SPG)**\n* Rebotes(totales) por juego **(RPG)**\n* Asistencias por partido **(APG)**\n\nTiene sentido los atributos obtenidos, ya que toma como atributos relevantes la cantidad de robos, rebotes y asistencias ya que se espera que un jugador perimetral participe más en jugadas de tipo defensivo así como en lanzamiento de 3 puntos, mientras que jugadores internos se podría esperar que la cantidad de rebotes ofensivos sea significativamente mayor."],"metadata":{}},{"cell_type":"code","source":["print model.stages[1].toDebugString\ntest_values=test.limit(3).select('features').toPandas().values\n\ndef print_features_from_data(data, list_of_features):\n  import numpy as np\n  aux=np.array([i[0][j] for i in data for j in list_of_features])\n  aux=aux.reshape((3,len(list_of_features)))\n  print pd.DataFrame(aux, columns=['feature'+str(i) for i in list_of_features], index=['sample'+str(i+1) for i in range(data.shape[0])])\n\nprint_features_from_data(test_values, [6,4,8,9,6,10])\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Teniendo en cuenta el anterior árbol y el listado de las características que usa el árbol lo podemos seguir para obtener los respectivos resultados de 3 ejemplos dados. A continuación se muestra las reglas seguidas para llegar a la predicción de cada ejemplo.\n\n1.\n  * Feature6 <= 1  (False)\n  * Feature10 <= 1.2 (True)\n  * Feature4 <=0.309 (True)\n  * **Predict 1**\n  \n2.\n  * Feature6 <=1 (True)\n  * Feature4 <=0 (False)\n  * Feature9 <=0.1 (False)\n  * **Predict 0**\n  \n3.\n  * Feature6 <=1 (True)\n  * Feature4 <=0 (Flse)\n  * Feature9 <=0.1 (False)\n  * **Predict 0**"],"metadata":{}},{"cell_type":"code","source":["display(model.transform(test.limit(3)).select('label','prediction'))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["# 4. Complejidad del modelo"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.feature import VectorIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\ndef func(md):\n  dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth = md)\n  pipeline = Pipeline(stages=[labelIndexer, dt])\n  model = pipeline.fit(train)\n  \n  predictions = model.transform(train)\n  accuracy = evaluator.evaluate(predictions)\n  t1 = 1.0-accuracy\n  \n  predictions = model.transform(test)\n  accuracy = evaluator.evaluate(predictions)\n  t2 = 1.0-accuracy\n  return (t1, t2, md)\n\nerrors = []\nfor i in range(1, 11):\n  p = func(i)\n  errors.append(p)\n  \ndf = sqlContext.createDataFrame(errors, [\"trainError\",\"testError\", \"depth\"])\ndisplay(df)\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["De acuerdo a la anterior gráfica el valor con menor complejidad pero que tiene buenos resultados en el conjunto de prueba es profundidad 3."],"metadata":{}},{"cell_type":"markdown","source":["# 5. Comparación de modelos"],"metadata":{}},{"cell_type":"code","source":["display(test)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier, NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\n\nnumFolds = 10\n\nlabelIndexer = StringIndexer(inputCol=\"class\", outputCol=\"label\").fit(data)\n\nnb = NaiveBayes(labelCol=\"label\", featuresCol=\"features\")\nf1_metric = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")  \npipeline = Pipeline(stages=[labelIndexer, nb])\ngrid = ParamGridBuilder().build()\nnb_crossval = CrossValidator(\n    estimator=pipeline,\n    estimatorParamMaps=grid,\n    evaluator=f1_metric,\n    numFolds=numFolds)\n\nnb_model = nb_crossval.fit(train)\nnb_prediction=nb_model.bestModel.transform(test)\n\nprint \"Naïve Bayes Classifier Metrics\"\nprint \"avg. F1-score\", nb_model.avgMetrics\nprint \"Best Model Metrics\"\nprint \"Accuracy: \",accuracy_metric.evaluate(nb_prediction)\nprint \"Recall:\", recall_metric.evaluate(nb_prediction)\nprint \"F1:\", f1_metric.evaluate(nb_prediction)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["numFolds = 10\n\nrf = RandomForestClassifier(numTrees=5, maxDepth=3, labelCol=\"label\", featuresCol=\"features\")\npipeline = Pipeline(stages=[labelIndexer, rf])\ngrid = ParamGridBuilder().build()\nrf_crossval = CrossValidator(\n    estimator=pipeline,\n    estimatorParamMaps=grid,\n    evaluator=f1_metric,\n    numFolds=numFolds)\n\nrf_model = rf_crossval.fit(train)\nrf_prediction=rf_model.bestModel.transform(test)\n\nprint \"Random Forest Classifier Metrics\"\nprint \"avg. F1-score\", rf_model.avgMetrics\nprint \"Best Model Metrics\"\nprint \"Accuracy: \",accuracy_metric.evaluate(rf_prediction)\nprint \"Recall:\", recall_metric.evaluate(rf_prediction)\nprint \"F1:\", f1_metric.evaluate(rf_prediction)\n"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Como se puede apreciar, el modelo de Random Forest fue mejor que el de Naïve Bayes, almenos para los parámetros establecidos para uno y otro, faltaría hacer una exploración de hiperparámetros para ambos modelos ya que existe la posibilidad de que la elección de estos para el modelo bayesiano no sean los mejores para el conjunto de datos usado."],"metadata":{}},{"cell_type":"markdown","source":["# 6. Cargue el conjunto de datos credit_german"],"metadata":{}},{"cell_type":"code","source":["data = sqlContext.read.csv(\"/FileStore/tables/m1jhk51i1479677154408/credit_german-8665c.csv\", header=True, inferSchema=True, sep=';')\ndisplay(data)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["columns = list(data.dtypes)\nfeatures = []\ndata = data.na.drop()\n\nfor col, typ in columns:\n  if col == \"class\":\n    continue\n  if typ == \"string\":\n    idxCol = col+\"Idx\"\n    feaCol = col+\"Feature\"\n    features.append(feaCol)\n    stringIndexer = StringIndexer(inputCol = col, outputCol = idxCol)\n    indexer = stringIndexer.fit(data)\n    data = indexer.transform(data)\n    \n    encoder = OneHotEncoder(dropLast = False, inputCol = idxCol, outputCol = feaCol)\n    data = encoder.transform(data)\n  else:\n    assembler = VectorAssembler(inputCols = [col], outputCol = str(col+\"Num\"))\n    data = assembler.transform(data)\n    features.append(col)\n    \nassembler = VectorAssembler(inputCols = features, outputCol = \"features\")\ndata = assembler.transform(data)\ndata = data.select(\"class\", \"features\")\ndisplay(data)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["(train, test) = data.randomSplit([0.7, 0.3])\n\nnumFolds = 10\n\nlabelIndexer = StringIndexer(inputCol=\"class\", outputCol=\"label\").fit(data)\n\nrf = RandomForestClassifier(numTrees=10, maxDepth=5, labelCol=\"label\", featuresCol=\"features\")\nf1_metric = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")  \npipeline = Pipeline(stages=[labelIndexer, rf])\ngrid = ParamGridBuilder().build()\nrf_crossval = CrossValidator(\n    estimator=pipeline,\n    estimatorParamMaps=grid,\n    evaluator=f1_metric,\n    numFolds=numFolds)\n\nrf_model = rf_crossval.fit(train)\nrf_prediction=rf_model.bestModel.transform(test)\n\nprint \"Random Forest Classifier Metrics\"\nprint \"avg. F1-score\", rf_model.avgMetrics\nprint \"Best Model Metrics\"\nprint \"Accuracy: \",accuracy_metric.evaluate(rf_prediction)\nprint \"Recall:\", recall_metric.evaluate(rf_prediction)\nprint \"F1:\", f1_metric.evaluate(rf_prediction)\n"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["display(rf_prediction)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["\nfrom pyspark.mllib.evaluation import MulticlassMetrics\n\nmetrics = MulticlassMetrics(rf_prediction.select('prediction', 'label').rdd)\n\nprint \"Confusion Matrix\"\nprint pd.DataFrame(metrics.confusionMatrix().toArray(), columns=['good', 'bad'], index=['good', 'bad'])\n\nprint \"Accuracy\", metrics.accuracy\nprint \"Precision\", metrics.precision(label=1)\nprint \"F1-Score\", metrics.fMeasure(label=1.0)\nprint \"Recall\", metrics.recall(label=1)\n"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Como se puede observar, a pesar que a primera vista podria verse que el accuracy es relativamente alto **(0.74)** lo que realmente nos podría interesar es clasificar los clientes de la categoría *\"bad\"* por lo que analizando métricas como el **Recall** podemos ver que es un mal modelo para esta tarea, lo que se refuerza aún más con el **F1-Score** que también es bajo. Por lo tanto este modelo debe ser descartado, podría hacerse una ajuste de parámetros intentando subir el Recall para la clase 1, pero no es alentador el horizonte al menos como se puede observar para el modelo descrito."],"metadata":{}}],"metadata":{"name":"Demo Supervisado Completar","notebookId":4365832148762269},"nbformat":4,"nbformat_minor":0}
