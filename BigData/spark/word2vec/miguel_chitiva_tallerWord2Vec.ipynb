{"cells":[{"cell_type":"code","source":["%sh wget \"https://raw.githubusercontent.com/imendibo/SEPLN-TASS15/master/DATA/general-tweets-train-tagged.xml\" & wget \"http://dis.unal.edu.co/~fgonza/courses/eswikinews.bin\""],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/databricks/driver/eswikinews.bin\", \"/FileStore/eswikinews.bin\")\ndbutils.fs.cp(\"file:/databricks/driver/general-tweets-train-tagged.xml\", \"/FileStore/general-tweets-train-tagged.xml\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%fs ls  \"/FileStore\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["# 1. Cargar archivo"],"metadata":{}},{"cell_type":"code","source":["df = sqlContext.read.format('com.databricks.spark.xml').options(rowTag='tweet').load('file:/databricks/driver/general-tweets-train-tagged.xml')\ndf.cache()\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["# 2. Extraer contenido y etiqueta de sentimiento"],"metadata":{}},{"cell_type":"code","source":["df=df.withColumn(\"sentiment_values\", df.sentiments.polarity.value)\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["# 3. Preprocesar contenidos de los tweets"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import udf, lower\nfrom pyspark.sql.types import *\nimport unicodedata, string\n\ndef remove_accents(input_str):\n  nfkd_form = unicodedata.normalize('NFKD', input_str)\n  only_ascii = nfkd_form.encode('ASCII', 'ignore')\n  return only_ascii\n\ndef remove_punctuation(input_str):\n  return input_str.translate(string.maketrans(\"\",\"\"), string.punctuation)\n  \n\n#to lowercase\ndf = df.withColumn(\"content\", lower(df.content))\n#remove accents\nudfReAccents = udf(remove_accents, StringType())\ndf = df.withColumn(\"content\", udfReAccents(\"content\"))\n#remove punctuation\nudfRePunct = udf(remove_punctuation, StringType())\ndf = df.withColumn(\"content\", udfRePunct(\"content\"))\ndisplay(df)\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["# 4. Preprocesar etiquetas de los sentimientos"],"metadata":{}},{"cell_type":"code","source":["#select only sentiment.polarity.entity = \"null\"\ndef nullInList(_list):\n  for i in _list:\n    if i == None or i == 'null' or i==\"NONE\":\n      return True\n  return False\n\nudfNullInList = udf(nullInList, BooleanType())\n\ndf=df.withColumn(\"sentiments_entity\", udfNullInList(\"sentiments.polarity.entity\"))\ndf=df.filter(df.sentiments_entity==True)\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["def convertPolarity(_list):\n  if _list[0] == \"P+\":\n    return \"P\"\n  elif _list[0] ==\"N+\":\n    return \"N\"\n  else:\n    return _list[0]\n\n#convert polarity, get first polarity, let only NEU, P, N polarities\nudfConvertP = udf(convertPolarity, StringType()) \ndf = df.withColumn(\"sentiment_values\", udfConvertP(\"sentiment_values\"))\ndf = df.filter(df.sentiment_values!='NONE')\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["# 5. Carga del modelo de Word2Vec"],"metadata":{}},{"cell_type":"code","source":["from gensim.models.word2vec import Word2Vec\nfrom pyspark.ml.feature import Tokenizer\nimport numpy as np\n\n#load word2vec model\nmodel = Word2Vec.load_word2vec_format('/dbfs/FileStore/eswikinews.bin', binary=True)\n\n#tokenize tweets\ntokenizer = Tokenizer(inputCol=\"content\", outputCol=\"words\")\ndf = tokenizer.transform(df)\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["# 6. Construcción del vector de características"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors, VectorUDT\nfrom pyspark.sql.functions import col\nimport numpy as np\n\ndef w2vec(_list):  \n  vec_list=[]\n  for i in _list:\n    try:\n      vec_list.append(model[i])\n    except KeyError:\n      pass\n  if not vec_list:\n    return None\n  return Vectors.dense(np.asarray(vec_list).mean(axis=0))\n\n#get features from tokens\nudfW2vec = udf(w2vec,  VectorUDT())\ndf = df.withColumn(\"features\", udfW2vec(\"words\"))\n#amount of null Tweets\nprint \"amount of null Tweets\", df.where(col(\"features\").isNull()).count()\ndf = df.where(col(\"features\").isNotNull())"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["display(df.select('words', 'features'))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["# 7. Clasificadores"],"metadata":{}},{"cell_type":"markdown","source":["## 7.1 Random Forest"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier, NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.feature import StringIndexer\n\n(train, test) = df.randomSplit([0.7, 0.3])\ntrain.cache()\ntest.cache()\n\n\naccuracy_metric = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\nrecall_metric = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nf1_metric = MulticlassClassificationEvaluator(\n    labelCol=\"label\", predictionCol=\"prediction\")\n#df.unpersist()\n\nnumFolds = 10\n\nlabelIndexer = StringIndexer(inputCol=\"sentiment_values\", outputCol=\"label\").fit(df)\n\nrf = RandomForestClassifier(numTrees=30, maxDepth=10, labelCol=\"label\", featuresCol=\"features\")\npipeline = Pipeline(stages=[labelIndexer, rf])\n\ngrid = ParamGridBuilder().build()\nrf_crossval = CrossValidator(\n    estimator=pipeline,\n    estimatorParamMaps=grid,\n    evaluator=f1_metric,\n    numFolds=numFolds)\n\nrf_model = rf_crossval.fit(train)\nrf_prediction=rf_model.bestModel.transform(test)\n\nprint \"Random Forest Classifier Metrics\"\nprint \"avg. F1-score\", rf_model.avgMetrics\nprint \"Best Model Metrics\"\nprint \"Accuracy: \",accuracy_metric.evaluate(rf_prediction)\nprint \"Recall:\", recall_metric.evaluate(rf_prediction)\nprint \"F1:\", f1_metric.evaluate(rf_prediction)\n"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["## 7.2 Mulilayer Perceptron"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import MultilayerPerceptronClassifier\n\nnumFolds = 10\n\nlabelIndexer = StringIndexer(inputCol=\"sentiment_values\", outputCol=\"label\").fit(df)\n\nlayers = [200, 100, 50, 20, 3]\n\nmlp = MultilayerPerceptronClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=300, layers=layers )\npipeline = Pipeline(stages=[labelIndexer, mlp])\n\ngrid = ParamGridBuilder().build()\nmlp_crossval = CrossValidator(\n    estimator=pipeline,\n    estimatorParamMaps=grid,\n    evaluator=f1_metric,\n    numFolds=numFolds)\n\nmlp_model = mlp_crossval.fit(train)\nmlp_prediction=mlp_model.bestModel.transform(test)\nprint \"Multi-Layer Perceptron\"\nprint \"avg. F1-score\", mlp_model.avgMetrics\nprint \"Best Model Metrics\"\nprint \"Accuracy: \",accuracy_metric.evaluate(mlp_prediction)\nprint \"Recall:\", recall_metric.evaluate(mlp_prediction)\nprint \"F1:\", f1_metric.evaluate(mlp_prediction)\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["# Fin \n___"],"metadata":{}}],"metadata":{"name":"word2vec-demo","notebookId":2740242735545006},"nbformat":4,"nbformat_minor":0}
